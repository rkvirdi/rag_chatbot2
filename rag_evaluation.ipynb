{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GursewakNeet\\AppData\\Local\\Temp\\ipykernel_36744\\3486159915.py:94: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  core_embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\GursewakNeet\\Documents\\rag_chatbot\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: In introducing the \"In Memoriam\" montage, Morgan Freeman also gave an individual spoken tribute to Gene Hackman.[75] The montage, which featured the Los Angeles Master Chorale performing \"Lacrimosa\" from Mozart's Requiem, paid tribute to the following individuals:[76]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GursewakNeet\\AppData\\Local\\Temp\\ipykernel_36744\\3486159915.py:31: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)[:k]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Evaluation Metrics: {'Precision@k': 0.2, 'Recall@k': 2.0, 'MRR': 0.5}\n",
      "Answer: The answer is Hugh Jackman and Ryan Reynolds. Hugh Jackman stars as Wolverine, and Ryan Reynolds stars as Deadpool.\n",
      "Retrieved Context: [Document(id='d99ed675-b80d-4013-af9c-6ae87f24b2a4', metadata={}, page_content='opportunity. A combo of Deadpool & Wolverine stars Hugh Jackman and Ryan Reynolds were also among those buzzed to be in the mix, but the latter told Deadline Hollywood that it was highly unlikely though he said he would like to host it with Jackman \"someday\".[8][40][41] As for the production team, returning to the telecast are Rob Paine as co-executive producer, Taryn Hurd and Sarah Levine Hall as producers, Mandy Moore as supervising choreographer, and Bob Dickinson and Noah Mitz as lighting'), Document(id='c8ab2065-e187-45b4-91b2-ed02d547f59a', metadata={}, page_content='opportunity. A combo of Deadpool & Wolverine stars Hugh Jackman and Ryan Reynolds were also among those buzzed to be in the mix, but the latter told Deadline Hollywood that it was highly unlikely though he said he would like to host it with Jackman \"someday\".[8][40][41] As for the production team, returning to the telecast are Rob Paine as co-executive producer, Taryn Hurd and Sarah Levine Hall as producers, Mandy Moore as supervising choreographer, and Bob Dickinson and Noah Mitz as lighting'), Document(id='a7cea902-0913-4e0e-b80e-dc4a8391eae9', metadata={}, page_content='opportunity. A combo of Deadpool & Wolverine stars Hugh Jackman and Ryan Reynolds were also among those buzzed to be in the mix, but the latter told Deadline Hollywood that it was highly unlikely though he said he would like to host it with Jackman \"someday\".[8][40][41] As for the production team, returning to the telecast are Rob Paine as co-executive producer, Taryn Hurd and Sarah Levine Hall as producers, Mandy Moore as supervising choreographer, and Bob Dickinson and Noah Mitz as lighting'), Document(id='6954e7ce-34bc-42a7-b612-dde45f004b9d', metadata={}, page_content='opportunity. A combo of Deadpool & Wolverine stars Hugh Jackman and Ryan Reynolds were also among those buzzed to be in the mix, but the latter told Deadline Hollywood that it was highly unlikely though he said he would like to host it with Jackman \"someday\".[8][40][41] As for the production team, returning to the telecast are Rob Paine as co-executive producer, Taryn Hurd and Sarah Levine Hall as producers, Mandy Moore as supervising choreographer, and Bob Dickinson and Noah Mitz as lighting')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableMap\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Load the dataset for evaluation\n",
    "with open(r\"C:\\Users\\GursewakNeet\\Documents\\rag_chatbot\\ground_truth.json\", \"r\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "# Function to calculate evaluation metrics\n",
    "def evaluate_retrieval(retriever, ground_truth, k=10):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for item in ground_truth:\n",
    "        query = item[\"query\"]\n",
    "        ground_truth_doc = item[\"ground_truth_document\"]\n",
    "\n",
    "        # Retrieve top-k documents\n",
    "        retrieved_docs = retriever.get_relevant_documents(query)[:k]\n",
    "\n",
    "        # Extract content for comparison\n",
    "        retrieved_content = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "        # Precision@k\n",
    "        relevant_retrieved = sum(1 for doc in retrieved_content if ground_truth_doc in doc)\n",
    "        precision = relevant_retrieved / k\n",
    "        precision_list.append(precision)\n",
    "\n",
    "        # Recall@k\n",
    "        total_relevant = 1  # Assuming one ground truth document\n",
    "        recall = relevant_retrieved / total_relevant\n",
    "        recall_list.append(recall)\n",
    "\n",
    "        # Mean Reciprocal Rank (MRR)\n",
    "        try:\n",
    "            rank = next(i + 1 for i, doc in enumerate(retrieved_content) if ground_truth_doc in doc)\n",
    "            reciprocal_ranks.append(1 / rank)\n",
    "        except StopIteration:\n",
    "            reciprocal_ranks.append(0)\n",
    "\n",
    "    # Calculate averages\n",
    "    precision_avg = sum(precision_list) / len(precision_list)\n",
    "    recall_avg = sum(recall_list) / len(recall_list)\n",
    "    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "    return {\n",
    "        \"Precision@k\": precision_avg,\n",
    "        \"Recall@k\": recall_avg,\n",
    "        \"MRR\": mrr\n",
    "    }\n",
    "\n",
    "### STEP 1: Scrape data and save as a JSON file ###\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://en.wikipedia.org/wiki/97th_Academy_Awards'\n",
    "\n",
    "response = requests.get(url)\n",
    "page_content = response.content\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "# Extract all <p> tags and combine their text\n",
    "paragraphs = soup.find_all('p')\n",
    "text_content = ' '.join([para.get_text() for para in paragraphs])\n",
    "#Import a text splitter class from LangChain.\n",
    "#This class recursively splits documents by paragraph, sentence, or character while preserving meaning.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#chunk_size=1000: Each chunk will be up to 1000 characters long.\n",
    "#chunk_overlap=20: Each chunk will overlap 20 characters with the next, helping maintain context between chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "chunks = text_splitter.split_text(text_content)\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "# Set up the cache store\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "# Initialize the Hugging Face embedding model\n",
    "core_embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Use the cache-backed embedder with the Hugging Face model\n",
    "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    core_embeddings_model,\n",
    "    store,\n",
    "    namespace=core_embeddings_model.model_name\n",
    ")\n",
    "\n",
    "# Store embeddings in the Pinecone vector store\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import os\n",
    "from pinecone import  Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()    \n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "index_name = \"ragchatbot2\" \n",
    "\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # Huggingface embeddings = 384\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "# Create vectorstore\n",
    "for i in range(0, len(chunks), 100):  # upload 100 at a time\n",
    "    vectorstore= PineconeVectorStore.from_texts(\n",
    "        chunks[i:i+100],\n",
    "        embedding=embedder,\n",
    "        index_name=index_name\n",
    "    )\n",
    "\n",
    "# Query\n",
    "query = \"What is Pinecone used for?\"\n",
    "results = vectorstore.similarity_search(query)\n",
    "\n",
    "# Print top result\n",
    "print(\"Answer:\", results[0].page_content)\n",
    "\n",
    "# Instantiate a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "import os\n",
    "# this formats the docs returned by the retriever\n",
    "def format_docs(docs):\n",
    "\treturn \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# prompt to send to the LLM\n",
    "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "    \tUse the following pieces of retrieved context to answer the question.\n",
    "    \tIf you don't know the answer, search in google  .\n",
    "\n",
    "    \tQuestion: {question}\n",
    "\n",
    "    \tContext: {context}\n",
    "\n",
    "    \tAnswer:\n",
    "    \t\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama3-70b-8192\", streaming=True, groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\"\"\"This code defines a chain where input documents are first formatted,\n",
    "then passed through a prompt template,\n",
    "and finally processed by an LLM.\"\"\"\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "\tRunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "\t\t| prompt_template\n",
    "\t| llm\n",
    "\t)\n",
    "\"\"\"This code creates a parallel process:\n",
    "one retrieves the context (using a retriever),\n",
    "and the other passes the question through unchanged.\n",
    "The results are then combined and assigned to the variable `answer` using the `rag_chain_from_docs` processing chain.\"\"\"\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "\t{\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "\n",
    "# Evaluate the retriever\n",
    "evaluation_metrics = evaluate_retrieval(retriever, ground_truth, k=10)\n",
    "print(\"Retrieval Evaluation Metrics:\", evaluation_metrics)\n",
    "\n",
    "# Run a query\n",
    "response = rag_chain_with_source.invoke(\"who stars in Deadpool & Wolverine\")\n",
    "print(\"Answer:\", response[\"answer\"].content)\n",
    "print(\"Retrieved Context:\", response[\"context\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a76fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Average BLEU Score': 0.1304316792244985, 'Average ROUGE Score': {'rouge-1': {'f': 0.6333333283333334, 'p': 0.6333333333333333, 'r': 0.6333333333333333}, 'rouge-2': {'f': 0.12499999750000004, 'p': 0.125, 'r': 0.125}, 'rouge-l': {'f': 0.6333333283333334, 'p': 0.6333333333333333, 'r': 0.6333333333333333}}}\n",
      "Answer: The answer is Hugh Jackman and Ryan Reynolds. Hugh Jackman stars as Wolverine, and Ryan Reynolds stars as Deadpool.\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "\n",
    "def evaluate_bleu_rouge(responses, ground_truths):\n",
    "    \"\"\"\n",
    "    Evaluate LLM-generated responses using BLEU and ROUGE scores.\n",
    "\n",
    "    Args:\n",
    "        responses (list of str): List of responses generated by the LLM.\n",
    "        ground_truths (list of str): List of ground truth answers.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing BLEU and ROUGE scores.\n",
    "    \"\"\"\n",
    "    assert len(responses) == len(ground_truths), \"Responses and ground truths must have the same length.\"\n",
    "\n",
    "    # Initialize scores\n",
    "    total_bleu_score = 0\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "\n",
    "    # Evaluate each response\n",
    "    for response, ground_truth in zip(responses, ground_truths):\n",
    "        # Compute BLEU score\n",
    "        bleu_score = sentence_bleu(\n",
    "            [ground_truth.split()], response.split(),\n",
    "            smoothing_function=SmoothingFunction().method1\n",
    "        )\n",
    "        total_bleu_score += bleu_score\n",
    "\n",
    "        # Compute ROUGE scores\n",
    "        rouge_score = rouge.get_scores(response, ground_truth, avg=True)\n",
    "        rouge_scores.append(rouge_score)\n",
    "\n",
    "    # Average BLEU score \n",
    "    avg_bleu_score = total_bleu_score / len(responses)\n",
    "\n",
    "    # Average ROUGE scores\n",
    "    avg_rouge_score = {\n",
    "        \"rouge-1\": {\n",
    "            \"f\": sum(score[\"rouge-1\"][\"f\"] for score in rouge_scores) / len(rouge_scores),\n",
    "            \"p\": sum(score[\"rouge-1\"][\"p\"] for score in rouge_scores) / len(rouge_scores),\n",
    "            \"r\": sum(score[\"rouge-1\"][\"r\"] for score in rouge_scores) / len(rouge_scores),\n",
    "        },\n",
    "        \"rouge-2\": {\n",
    "            \"f\": sum(score[\"rouge-2\"][\"f\"] for score in rouge_scores) / len(rouge_scores),\n",
    "            \"p\": sum(score[\"rouge-2\"][\"p\"] for score in rouge_scores) / len(rouge_scores),\n",
    "            \"r\": sum(score[\"rouge-2\"][\"r\"] for score in rouge_scores) / len(rouge_scores),\n",
    "        },\n",
    "        \"rouge-l\": {\n",
    "            \"f\": sum(score[\"rouge-l\"][\"f\"] for score in rouge_scores) / len(rouge_scores),\n",
    "            \"p\": sum(score[\"rouge-l\"][\"p\"] for score in rouge_scores) / len(rouge_scores),\n",
    "            \"r\": sum(score[\"rouge-l\"][\"r\"] for score in rouge_scores) / len(rouge_scores),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"Average BLEU Score\": avg_bleu_score,\n",
    "        \"Average ROUGE Score\": avg_rouge_score,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "responses = [\n",
    "    \"This is a generated response.\",\n",
    "    \"Another generated response.\"\n",
    "]\n",
    "ground_truths = [\n",
    "    \"This is the expected response.\",\n",
    "    \"Another correct response.\"\n",
    "]\n",
    "\n",
    "scores = evaluate_bleu_rouge(responses, ground_truths)\n",
    "print(scores)\n",
    "print(\"Answer:\", response[\"answer\"].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
